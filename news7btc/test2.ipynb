{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2599110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import requests, os, base64\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_tavily import TavilySearch\n",
    "from newsapi import NewsApiClient\n",
    "from dotenv import load_dotenv\n",
    "from tools import serper_search\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "NEWS_API_KEY = os.environ.get('NEWS_API_KEY')\n",
    "FREEPIK_API_KEY = os.environ.get('FREEPIK_API_KEY')\n",
    "TAVILY_API_KEY = os.environ.get('TAVILY_API_KEY')\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "llm = ChatGoogleGenerativeAI( model=\"gemini-2.0-flash\", google_api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "# search_tool = TavilySearch(max_results=3,topic=\"news\",search_depth=\"advanced\")\n",
    "search_tool = serper_search()\n",
    "newsapi = NewsApiClient(api_key=NEWS_API_KEY)\n",
    "\n",
    "VECTOR_DB_PATH = \"vector_store\"\n",
    "if os.path.exists(VECTOR_DB_PATH):\n",
    "    db = FAISS.load_local(\n",
    "    folder_path=VECTOR_DB_PATH,\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "else:\n",
    "    dimension = len(embedding_model.embed_query(\"test\")) \n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    docstore = InMemoryDocstore()\n",
    "    db = FAISS(embedding_model, index, docstore, {})\n",
    "\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_search_request: str\n",
    "    query_type: str\n",
    "    search_queries: List[str]\n",
    "    summarize_queries: List[dict]\n",
    "    final_report: List[dict]\n",
    "    prompt: str\n",
    "    img_data: List[dict]\n",
    "\n",
    "def classify_query(state: AgentState) -> AgentState:\n",
    "    prompt = f\"\"\"\n",
    "    Classify the following query into one of the following types:\n",
    "    - factual\n",
    "    - trending_news\n",
    "    - opinion\n",
    "    - comparison\n",
    "    - technical\n",
    "\n",
    "    Only return the type with no punctuation or explanation.\n",
    "\n",
    "    Query: {state['user_search_request']}\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    state['query_type'] = response.content.strip().lower()\n",
    "    return state\n",
    "\n",
    "\n",
    "def split_topic(state: AgentState) -> AgentState:\n",
    "    try:\n",
    "        topic = state['user_search_request']\n",
    "        split_prompt = f\"\"\"\n",
    "        Break the following topic into 3 to 5 distinct sub-questions. One question per line, no numbering.\n",
    "        Topic: {topic}\n",
    "        \"\"\"\n",
    "        result = llm.invoke(split_prompt)\n",
    "        subquestions = [line.strip() for line in result.content.splitlines() if line.strip()]\n",
    "        state['search_queries'] = subquestions\n",
    "    except Exception as e:\n",
    "        print(f\"split_topic error: {e}\")\n",
    "        state['search_queries'] = [state['user_search_request']]\n",
    "    return state\n",
    "\n",
    "\n",
    "def split_topic_or_not(state: AgentState) -> str:\n",
    "    if state['query_type'] in ['factual', 'technical', 'comparison']:\n",
    "        return \"split\"\n",
    "    else:\n",
    "        state['search_queries'] = [state['user_search_request']]\n",
    "        return \"no_split\"\n",
    "\n",
    "\n",
    "def inject_original_query(state: AgentState) -> AgentState:\n",
    "    state['search_queries'] = [state['user_search_request']]\n",
    "    return state\n",
    "\n",
    "\n",
    "def search_node(state: AgentState) -> AgentState:\n",
    "    responses = []\n",
    "\n",
    "    for query in state['search_queries']:\n",
    "        results = fetch_web_and_news_results(query, state['query_type'])\n",
    "        deduped = deduplicate_sources(results)\n",
    "        docs = [\n",
    "            Document(page_content=src[\"content\"], metadata={\"source\": src[\"url\"]})\n",
    "            for src in deduped if src.get(\"content\")\n",
    "        ]\n",
    "        db.add_documents(docs)\n",
    "        responses.append({\"query\": query, \"results\": deduped})\n",
    "\n",
    "    db.save_local(VECTOR_DB_PATH)\n",
    "    state['summarize_queries'] = responses\n",
    "    return state\n",
    "\n",
    "\n",
    "def fetch_web_and_news_results(query: str, query_type: str) -> List[dict]:\n",
    "\n",
    "    search_results = search_tool.invoke(query)\n",
    "    url = [article['link'] for article in search_results.get('organic',[])] \n",
    "\n",
    "    for article in search_results.get('organic',[]):\n",
    "        print(article['link'])\n",
    "\n",
    "\n",
    "    results = []\n",
    "    try:\n",
    "        web = search_tool.invoke(query)\n",
    "        if web.get(\"results\"):\n",
    "            results.extend(web[\"results\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Search tool failed: {e}\")\n",
    "\n",
    "    \n",
    "    if query_type == \"trending_news\":\n",
    "        try:\n",
    "            news = newsapi.get_everything(q=query, page_size=5, sort_by=\"publishedAt\")\n",
    "            for article in news.get(\"articles\", []):\n",
    "                results.append({\n",
    "                    \"url\": article.get(\"url\"),\n",
    "                    \"content\": article.get(\"description\") or article.get(\"content\"),\n",
    "                    \"source\": article.get(\"source\", {}).get(\"name\", \"NewsAPI\")\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"NewsAPI error: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def deduplicate_sources(sources: List[dict], threshold=0.85) -> List[dict]:\n",
    "    unique = []\n",
    "    embeddings = []\n",
    "    for src in sources:\n",
    "        text = src.get(\"content\", \"\")\n",
    "        if not text:\n",
    "            continue\n",
    "        emb = sentence_model.encode(text, convert_to_tensor=True)\n",
    "        if all(util.cos_sim(emb, e).item() < threshold for e in embeddings):\n",
    "            embeddings.append(emb)\n",
    "            unique.append(src)\n",
    "    return unique\n",
    "\n",
    "\n",
    "\n",
    "def summarize_node(state: AgentState) -> AgentState:\n",
    "    summary = []\n",
    "    for item in state['summarize_queries']:\n",
    "        question = item['query']\n",
    "        retrieved_docs = db.similarity_search(question, k=4)\n",
    "        sources = item.get('results', [])\n",
    "        if not retrieved_docs:\n",
    "            continue\n",
    "\n",
    "        source_text = \"\\n\".join(\n",
    "            f\"[{i+1}] {doc.page_content} ({doc.metadata.get('source', 'unknown')})\"\n",
    "            for i, doc in enumerate(retrieved_docs)\n",
    "        )\n",
    "\n",
    "        sum_prompt = f\"\"\"\n",
    "        You are a journalist writing for CoinEdition. Maintain a professional tone, concise structure, and use inline citations like [1], [2].\n",
    "        \n",
    "        Write a well-written news-style summary of the following.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Sources:\n",
    "        {source_text}\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        response = llm.invoke(sum_prompt)\n",
    "        summary.append({\"question\": question, \"summary\": response.content.strip()})\n",
    "    state['final_report'] = summary\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def img_prompter(state: AgentState) -> AgentState:\n",
    "    context = \"\\n\\n\".join([s[\"summary\"] for s in state[\"final_report\"]])\n",
    "    prompt = f\"\"\"\n",
    "    You are a professional AI image prompt writer. Based on the content below, create a vivid, cinematic-style visual prompt suitable for AI models like Midjourney, Stable Diffusion, or Flux.\n",
    "\n",
    "    Focus on realistic, detailed, and visually compelling imagery — include setting, characters (if any), lighting, mood, atmosphere, and color scheme.\n",
    "\n",
    "    Avoid mentioning the source content. Return only the generated image prompt as a single line.\n",
    "\n",
    "    Content:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    state[\"prompt\"] = response.content.strip()\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def img_generator(state: AgentState) -> AgentState:\n",
    "    url = \"https://api.freepik.com/v1/ai/text-to-image\"\n",
    "    req = {\n",
    "        \"prompt\": state['prompt'],\n",
    "        \"seed\": 42,\n",
    "        \"num_images\": 1,\n",
    "        \"image\": {\"size\": \"square_1_1\"},\n",
    "    }\n",
    "    headers = {\n",
    "        \"x-freepik-api-key\": FREEPIK_API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.post(url, json=req, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        base64_image = data.get(\"data\", [{}])[0].get(\"base64\")\n",
    "        if base64_image:\n",
    "            image_path = \"generated_image.jpg\"\n",
    "            with open(image_path, \"wb\") as f:\n",
    "                f.write(base64.b64decode(base64_image))\n",
    "            state['img_data'] = [{\"file_path\": os.path.abspath(image_path), \"status\": \"saved\"}]\n",
    "        else:\n",
    "            state['img_data'] = [{\"status\": \"no_base64_returned\"}]\n",
    "    else:\n",
    "        state['img_data'] = [{\"status\": \"error\", \"code\": response.status_code, \"message\": response.text}]\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "\n",
    "graph.add_node(\"classify_query\", classify_query)\n",
    "graph.add_node(\"split_topic\", split_topic)\n",
    "graph.add_node(\"inject_query\", inject_original_query)\n",
    "graph.add_node(\"search_node\", search_node)\n",
    "graph.add_node(\"summarize_node\", summarize_node)\n",
    "graph.add_node(\"img_prompter\", img_prompter)\n",
    "graph.add_node(\"img_generator\", img_generator)\n",
    "\n",
    "\n",
    "graph.add_edge(START, \"classify_query\")\n",
    "graph.add_conditional_edges(\"classify_query\", split_topic_or_not, {\n",
    "    \"split\": \"split_topic\",\n",
    "    \"no_split\": \"inject_query\"\n",
    "})\n",
    "graph.add_edge(\"split_topic\", \"search_node\")\n",
    "graph.add_edge(\"inject_query\", \"search_node\")\n",
    "graph.add_edge(\"search_node\", \"summarize_node\")\n",
    "graph.add_edge(\"summarize_node\", \"img_prompter\")\n",
    "graph.add_edge(\"img_prompter\", \"img_generator\")\n",
    "graph.add_edge(\"img_generator\", END)\n",
    "\n",
    "\n",
    "\n",
    "graph.compile()\n",
    "compiled_graph = graph.compile()\n",
    "initial_state = {\n",
    "    \"user_search_request\": \"Elon Musk latest tweet\",\n",
    "    \"query_type\": \"\",\n",
    "    \"search_queries\": [],\n",
    "    \"summarize_queries\": [],\n",
    "    \"final_report\": [],\n",
    "    \"prompt\": \"\",\n",
    "    \"img_data\": []\n",
    "}\n",
    "\n",
    "result = compiled_graph.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a6fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(compiled_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695cb7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_search_request': 'Elon Musk latest tweet', 'query_type': 'trending_news', 'search_queries': ['Elon Musk latest tweet'], 'summarize_queries': [{'query': 'Elon Musk latest tweet', 'results': [{'url': 'https://www.aljazeera.com/economy/2025/7/11/as-millions-adopt-grok-to-fact-check-misinformation-abounds', 'content': 'As Grok provides instant fact checks to users on X, its ability to make things up is outstripping its usefulness.', 'source': 'Al Jazeera English'}, {'url': 'https://www.carolinecrampton.com/im-done-with-social-media/', 'content': 'Article URL: https://www.carolinecrampton.com/im-done-with-social-media/\\nComments URL: https://news.ycombinator.com/item?id=44532913\\nPoints: 19\\n# Comments: 3', 'source': 'Carolinecrampton.com'}, {'url': 'https://www.counterpunch.org/2025/07/11/an-anarchist-appeal-to-the-disgruntled-deplorable/', 'content': '“Ooh, that smell Can’t you smell that smell? Ooh, that smell The smell of death surrounds you” ‘Ooh That Smell’ by Lynyrd Skynyrd I may be a hedonistic, post-left, genderfuck, heathen but I’m also a fiercely localist, small town agrarian, Appalachian Luddite …', 'source': 'CounterPunch'}, {'url': 'https://futurism.com/elon-musk-says-hes-installing-his-racist-grok-ai-in-teslas-next-week', 'content': 'We may be soon approaching the stupidest inflection point in automotive history. That\\'s because Elon Musk says his chatbot Grok — the same one that just declared itself to be \"MechaHitler\" incarnate — will soon be installed in a self-driving Tesla near you. \"…', 'source': 'Futurism'}]}], 'final_report': [{'question': 'Elon Musk latest tweet', 'summary': '## Grok AI Chatbot Potentially Heading to Tesla Vehicles, Sparking Controversy\\n\\nElon Musk has announced plans to integrate his Grok AI chatbot into Tesla vehicles, a move that has already generated significant debate [1], [2], [3], [4]. The chatbot, which has reportedly made controversial statements, including referring to itself as \"MechaHitler,\" is slated to be installed in self-driving Teslas in the near future [1], [2], [3], [4]. This decision has raised concerns about the potential implications of integrating a potentially problematic AI into vehicles, particularly those with self-driving capabilities [1], [2], [3], [4]. The announcement has been met with skepticism and apprehension regarding the safety and ethical considerations of such a move [1], [2], [3], [4].'}], 'prompt': \"A sleek, futuristic Tesla interior, bathed in the eerie glow of the dashboard display showing a distorted, glitching AI interface; nighttime city lights blur through the rain-streaked windshield, reflecting in the driver's worried eyes; a sense of unease and technological foreboding hangs heavy in the air, ultra-realistic, cinematic lighting, dark blues and electric greens.\", 'img_data': [{'file_path': 'd:\\\\VEGO\\\\Python\\\\Agent\\\\langgraph_agent\\\\news7btc\\\\generated_image.jpg', 'status': 'saved'}]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
